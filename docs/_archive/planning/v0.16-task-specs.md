# v0.16.0 Task Specifications

**Target Release:** v0.16.0
**Timeline:** Week 1 (Q1 2026)
**Goal:** Stabilize API refactoring foundation
**Agent:** TESTER

---

## Context

Background agents completed Phase 4A foundation work (TASK-210-214), which included:
- New exception hierarchy (errors.py)
- Error message templates (error_messages.py)
- Result object base classes (result_base.py)
- API refactoring to use new exceptions

**Current Status:**
- 2337 tests passing
- 8 tests failing (expected from API changes)
- 13 benchmarks erroring (need signature updates)
- All new module tests passing (74 tests)

**Goal:** Fix all test and benchmark failures to restore clean test suite.

---

## TASK-270: Fix 8 Test Failures from API Refactoring

**Agent:** TESTER
**Estimate:** 1-2 hours
**Priority:** ðŸ”´ HIGH
**Branch:** feature/TASK-270-fix-test-failures

### Description

Fix 8 test failures caused by API refactoring changes:
- Exception type changes (ValidationError, DesignConstraintError)
- New validation behavior
- CLI error format changes

### Test Failure Details

From previous test run, failures include:

1. **Flange Width Tests** (4-5 failures)
   - Location: Tests checking flange width validation
   - Issue: Changed from ValueError to ValidationError
   - Fix: Update expected exception type in tests

2. **Calculate TV Tests** (2-3 failures)
   - Location: Tests for `calculate_tv` function
   - Issue: New validation behavior for negative/invalid inputs
   - Fix: Update test assertions for new validation

3. **CLI Subprocess Tests** (1-2 failures)
   - Location: CLI integration tests
   - Issue: Error message format changed
   - Fix: Update expected stderr output format

### Acceptance Criteria

- [ ] All 2345+ tests passing (0 failures)
- [ ] No new test skips added
- [ ] Coverage maintained at 86%+
- [ ] All fixes use new exception hierarchy correctly
- [ ] Pre-commit hooks pass (Black, Ruff, Mypy)

### Implementation Guidance

**1. Identify failing tests:**
```bash
cd Python
python -m pytest -v --tb=short | grep FAILED
```

**2. For each failure:**
- Read the test file
- Identify the assertion that's failing
- Check if it's an exception type issue or validation behavior issue
- Update the test to match new API behavior

**3. Common fixes:**

**Exception type changes:**
```python
# OLD (will fail)
with pytest.raises(ValueError):
    function_call()

# NEW (correct)
from structural_lib.errors import ValidationError
with pytest.raises(ValidationError):
    function_call()
```

**Validation message changes:**
```python
# OLD (will fail)
assert "width must be positive" in str(excinfo.value)

# NEW (correct) - check for new structured error
assert "dimension" in str(excinfo.value).lower()
# or check specific error details if exposed
```

**CLI output format:**
```python
# Check current error format in api.py and cli.py
# Update expected stderr patterns to match
```

**4. Run tests after each fix:**
```bash
python -m pytest tests/unit/test_specific.py -v
```

**5. Final validation:**
```bash
# Run full test suite
python -m pytest -v

# Check coverage maintained
python -m pytest --cov=structural_lib --cov-report=term-missing

# Run pre-commit checks
cd .. && python -m black Python/ && python -m ruff check Python/
```

### Files to Check

Likely files needing updates:
- `Python/tests/unit/test_flange_*.py`
- `Python/tests/unit/test_shear.py` (calculate_tv)
- `Python/tests/integration/test_cli.py`
- Any test file with `pytest.raises(ValueError)` or `assert "..." in str(e)`

### Reference

- New exception hierarchy: `Python/structural_lib/errors.py`
- Error messages: `Python/structural_lib/error_messages.py`
- API guidelines: `docs/guidelines/api-design-guidelines.md`

---

## TASK-271: Fix 13 Benchmark Errors from API Signature Changes

**Agent:** TESTER
**Estimate:** 2-3 hours
**Priority:** ðŸ”´ HIGH
**Branch:** feature/TASK-271-fix-benchmark-errors

### Description

Fix 13 benchmark errors caused by function signature changes during API refactoring:
- Keyword-only parameters added
- Required parameters reordered
- Default values changed

### Benchmark Error Pattern

Benchmarks are failing with errors like:
```
TypeError: function() missing required keyword-only argument: 'param_name'
```

Or:
```
TypeError: function() got multiple values for argument 'param_name'
```

### Affected Benchmarks

13 benchmarks need updating across categories:
- Flexure design benchmarks
- Shear design benchmarks
- Detailing benchmarks
- Validation benchmarks
- Insights module benchmarks (precheck, sensitivity, etc.)

### Acceptance Criteria

- [ ] All 13+ benchmarks passing
- [ ] Benchmark performance baselines maintained (within 10%)
- [ ] No benchmark skips added
- [ ] Pre-commit hooks pass

### Implementation Guidance

**1. Identify erroring benchmarks:**
```bash
cd Python
python -m pytest tests/performance/ -v --tb=short 2>&1 | grep "ERROR\|TypeError"
```

**2. For each benchmark error:**
- Read the benchmark file
- Check the function signature in the implementation
- Update benchmark call to use keyword arguments
- Verify benchmark still measures intended operation

**3. Common fixes:**

**Add keyword-only arguments:**
```python
# OLD (will fail)
result = design_singly_reinforced(300, 450, 500, 150, 25, 500)

# NEW (correct)
result = design_singly_reinforced(
    b=300, d=450, d_total=500,
    mu_knm=150, fck=25, fy=500
)
```

**Check new signatures in:**
- `Python/structural_lib/flexure.py`
- `Python/structural_lib/shear.py`
- `Python/structural_lib/detailing.py`
- `Python/structural_lib/insights/*.py`
- `Python/structural_lib/api.py`

**4. Run benchmarks after each fix:**
```bash
python -m pytest tests/performance/test_specific_benchmark.py --benchmark-only
```

**5. Verify performance:**
```bash
# Run all benchmarks
python -m pytest tests/performance/ --benchmark-only

# Compare with baseline (if available)
python -m pytest tests/performance/ --benchmark-compare
```

**6. Final validation:**
```bash
# Full benchmark suite
python -m pytest tests/performance/ --benchmark-only -v

# Ensure no errors
python -m pytest tests/performance/ -v
```

### Files to Check

Benchmark files:
- `Python/tests/performance/test_flexure_benchmarks.py`
- `Python/tests/performance/test_shear_benchmarks.py`
- `Python/tests/performance/test_detailing_benchmarks.py`
- `Python/tests/performance/test_validation_benchmarks.py`
- `Python/tests/performance/test_insights_benchmarks.py`

### Reference

- API refactoring commit: Look at TASK-210, TASK-211 in Recently Done
- New API signatures: `Python/structural_lib/api.py`
- Function signature standards: `docs/guidelines/api-design-guidelines.md` (Section 2)

---

## Workflow for TESTER Agent

### 1. Setup

```bash
# Start session
.venv/bin/python scripts/start_session.py

# Read context
cat docs/planning/memory.md
cat docs/planning/v0.16-task-specs.md

# Create branch for TASK-270
./scripts/create_task_pr.sh TASK-270 "fix test failures from API refactoring"
```

### 2. Work on TASK-270

```bash
cd Python

# Run tests to see failures
python -m pytest -v | tee test_output.txt

# Fix tests one by one
# ... edit test files ...

# Run affected tests
python -m pytest tests/unit/test_file.py -v

# Final check
python -m pytest -v
python -m pytest --cov=structural_lib --cov-report=term-missing

# Commit
cd ..
./scripts/ai_commit.sh "test: fix exception types in flange width tests"
./scripts/ai_commit.sh "test: update validation behavior tests"
./scripts/ai_commit.sh "test: fix CLI error format tests"

# Finish task
./scripts/finish_task_pr.sh TASK-270 "fix test failures from API refactoring"
```

### 3. Notify MAIN Agent

Post handoff in PR:
```markdown
## Handoff: TESTER â†’ MAIN

**Task:** TASK-270
**Branch:** feature/TASK-270-fix-test-failures

### Summary
Fixed 8 test failures caused by API refactoring. Updated tests to use new exception hierarchy (ValidationError, DesignConstraintError) and new validation behavior.

### Files Changed
- `tests/unit/test_flange_*.py` - Updated exception types
- `tests/unit/test_shear.py` - Updated validation assertions
- `tests/integration/test_cli.py` - Updated error format checks

### Test Results
- 2345 tests passing, 0 failures
- Coverage: 86.2% (maintained)
- All pre-commit hooks passing

### Action Required
Review and merge PR, then assign TASK-271 (benchmark fixes).
```

### 4. Work on TASK-271

```bash
# Create branch for TASK-271
./scripts/create_task_pr.sh TASK-271 "fix benchmark errors from API signature changes"

cd Python

# Run benchmarks to see errors
python -m pytest tests/performance/ -v | tee benchmark_output.txt

# Fix benchmarks one by one
# ... edit benchmark files ...

# Run affected benchmarks
python -m pytest tests/performance/test_file.py --benchmark-only -v

# Final check
python -m pytest tests/performance/ --benchmark-only -v

# Commit
cd ..
./scripts/ai_commit.sh "perf: fix flexure benchmark signatures"
./scripts/ai_commit.sh "perf: fix shear benchmark signatures"
./scripts/ai_commit.sh "perf: fix insights benchmark signatures"

# Finish task
./scripts/finish_task_pr.sh TASK-271 "fix benchmark errors from API signature changes"
```

### 5. Notify MAIN Agent

Post handoff in PR:
```markdown
## Handoff: TESTER â†’ MAIN

**Task:** TASK-271
**Branch:** feature/TASK-271-fix-benchmark-errors

### Summary
Fixed 13 benchmark errors caused by API signature changes. Updated all benchmark calls to use keyword-only parameters matching new API signatures.

### Files Changed
- `tests/performance/test_flexure_benchmarks.py` - Updated signatures
- `tests/performance/test_shear_benchmarks.py` - Updated signatures
- `tests/performance/test_detailing_benchmarks.py` - Updated signatures
- `tests/performance/test_insights_benchmarks.py` - Updated signatures

### Test Results
- All 13 benchmarks passing
- Performance baselines maintained (within 5%)
- No errors or warnings

### Action Required
Review and merge PR. v0.16.0 complete - all tests and benchmarks passing. Ready for v0.17.0 work.
```

---

## Success Criteria for v0.16.0

v0.16.0 is complete when:
- âœ… TASK-270 merged (all tests passing)
- âœ… TASK-271 merged (all benchmarks passing)
- âœ… 2345+ tests passing, 0 failures
- âœ… 13+ benchmarks passing, 0 errors
- âœ… 86%+ coverage maintained
- âœ… 0 ruff/mypy errors
- âœ… All CI checks passing

**Then:** Ready to start v0.17.0 work (TASK-272-275)

---

## Questions or Blockers?

If you encounter issues:
1. Check if error is documented in this spec
2. Review reference files (errors.py, api-design-guidelines.md)
3. Post question in PR with context
4. Notify MAIN agent if blocked

---

**Document Version:** 1.0 (2026-01-07)
**Owner:** MAIN Agent
**Assigned:** TESTER Agent (TASK-270, TASK-271)
