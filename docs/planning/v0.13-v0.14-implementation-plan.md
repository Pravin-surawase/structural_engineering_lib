# v0.13 & v0.14 Implementation Plan

**Purpose:** Detailed implementation plan for intelligence features and deterministic alternatives
**Focus:** Stability, safety, robustness
**Created:** 2025-12-30
**Status:** Planning

**Safety Rails (non-negotiable):**
- Advisory only: smart features never change pass/fail or required steel.
- No changes to stable core/orchestration APIs in v0.13/v0.14.
- Deterministic outputs only (no hidden randomness).
- No mandatory new dependencies in v0.13/v0.14.
- No schema changes by default; insights output is separate unless a schema bump is approved.

---

## Overview

### Vision

Transform the library from a **validation engine** to a **design intelligence platform** while maintaining:
- ✅ Deterministic outputs (no ML black boxes)
- ✅ IS 456 compliance (all results traceable to clauses)
- ✅ API stability (no breaking changes)
- ✅ Engineering safety (conservative by default)
- ✅ Advisory-only insights (never override design results)
- ✅ Library-first (no UI/product features)

### Release Sequence

```
v0.12.0 (Current)
    ↓
v0.13.0 — Intelligence Foundations (3-4 weeks)
    ├─ Predictive validation
    ├─ Sensitivity analysis
    ├─ Constructability scoring
    └─ Enhanced verification (5-10 benchmark cases)
    ↓
v0.14.0 — Deterministic Alternatives (3-4 weeks)
    ├─ Enumerated design alternatives (deterministic search)
    ├─ Pareto front selection (no stochastic solvers)
    ├─ Detailing completeness (anchorage space check)
    └─ Enhanced error hints (advisory only)
```

---

# Part 1: v0.13 — Intelligence Foundations

**Timeline:** 3-4 weeks
**Scope:** Foundation features for smart design assistance
**Risk Level:** Low (simple, deterministic algorithms)

---

## v0.13 Architecture

### Module Structure

```
structural_lib/
├── insights/                # NEW: Advisory-only intelligence (opt-in)
│   ├── __init__.py           # Not imported by structural_lib/__init__.py
│   ├── precheck.py
│   ├── sensitivity.py
│   ├── constructability.py
│   └── types.py              # Insight-only dataclasses
│
├── verification/            # NEW: Benchmark corpus (data + runner)
│   ├── __init__.py
│   ├── benchmark_cases.py   # 5-10 verified examples
│   └── validate.py          # Validation runner
│
├── flexure.py              # UNCHANGED (stable API)
├── shear.py                # UNCHANGED
├── api.py                  # UNCHANGED (no new flags in v0.13)
└── types.py                # UNCHANGED (no new fields in v0.13)
```

### Design Principles

**1. Separation of Concerns**
```python
# Core design functions remain pure
result = design_singly_reinforced(...)  # No changes

# Insights features are opt-in addons
precheck = quick_precheck(...)          # Optional pre-step
sensitivities = sensitivity_analysis(...) # Optional post-step
```

**2. Backward Compatibility**
```python
# Old code continues to work
result = design_beam_is456(units="IS456", ...)  # ✅ Works

# New features are separate, opt-in utilities
from structural_lib.insights import precheck, sensitivity
pre = precheck.quick_precheck(...)
sens, robust = sensitivity.sensitivity_analysis(...)
```

**3. Fail-Safe Defaults**
```python
# If insight features fail, design still works
try:
    precheck = quick_precheck(...)
except Exception as e:
    logger.warning(f"Precheck failed: {e}")
    precheck = None  # Continue without precheck
```

---

## v0.13 Implementation Details

### Feature 1: Predictive Validation

**Module:** `structural_lib/insights/precheck.py`

**API:**
```python
from structural_lib.errors import Severity
from structural_lib.types import SupportCondition

@dataclass(frozen=True)
class HeuristicWarning:
    """Warning from heuristic pre-check."""
    type: str  # "deflection_risk", "steel_congestion", etc.
    severity: Severity  # INFO or WARNING (never ERROR)
    message: str  # Human-readable issue
    suggestion: str  # Actionable fix
    rule_basis: str  # IS 456 reference or engineering principle

@dataclass(frozen=True)
class PredictiveCheckResult:
    """Results from quick heuristic validation."""
    check_time_ms: float
    risk_level: str  # "LOW", "MEDIUM", "HIGH"
    warnings: list[HeuristicWarning]
    recommended_action: str  # "proceed", "review_geometry", "increase_depth"
    heuristics_version: str  # "1.0" for tracking

def quick_precheck(
    span_mm: float,
    b_mm: float,
    d_mm: float,
    D_mm: float,
    mu_knm: float,
    fck_nmm2: float,
    fy_nmm2: float = 500.0,
    support_condition: SupportCondition = SupportCondition.SIMPLY_SUPPORTED,
) -> PredictiveCheckResult:
    """
    Fast heuristic validation before full design.

    Target: < 10ms execution time (soft target; perf tests are non-blocking)
    Basis: Engineering rules of thumb + IS 456 guidance

    Rules:
    1. Span/depth ratio check (Table 23 guidance, or deflection check if inputs allow)
    2. Steel percentage estimate (lever arm approximation)
    3. Width adequacy (constructability)
    4. Cover/depth sanity check
    5. Concrete grade validation (flag out-of-range for Table 19 use)

    Returns:
        Warnings if potential issues detected, empty list if OK

    Safety Note:
        This is NOT a code check - only heuristic guidance.
        Always run full design for compliance.
    """
```

**Heuristic Rules (Conservative by Default):**

| Rule | Threshold | Action if Exceeded | IS 456 Basis |
|------|-----------|-------------------|--------------|
| Span/depth ratio | Exceeds Table 23 guidance | Flag deflection risk | Table 23 (span/depth) |
| Steel estimate | > 4% | Flag doubly reinforced likely | Practical limit for singly |
| Width | < 150mm | Flag spacing issues | Constructability |
| Cover/D ratio | > 0.25 | Flag possible input error | Typical: 10-15% |
| fck range | Outside 15-40 | Flag Table 19 clamping risk | Table 19 range |

**Implementation Safety:**

1. **Conservative thresholds** - Bias toward false positives (safe)
2. **Clear disclaimers** - "Heuristic only, not code check"
3. **Versioning** - Track heuristics version for reproducibility
4. **Advisory only** - No effect on pass/fail or required steel
5. **Fallback** - If precheck fails, doesn't block design

**Testing Strategy:**

```python
# tests/test_insights_precheck.py
import pytest
from structural_lib.errors import Severity

def test_precheck_deflection_risk():
    """Shallow beam should trigger deflection warning."""
    result = quick_precheck(
        span_mm=6000, b_mm=230, d_mm=250,  # span/d = 24
        D_mm=300, mu_knm=120, fck_nmm2=25
    )
    assert result.risk_level == "HIGH"
    assert any(w.type == "deflection_risk" for w in result.warnings)

def test_precheck_normal_beam():
    """Typical beam should pass."""
    result = quick_precheck(
        span_mm=5000, b_mm=300, d_mm=450,  # span/d = 11
        D_mm=500, mu_knm=120, fck_nmm2=25
    )
    assert result.risk_level == "LOW"
    assert len(result.warnings) == 0

@pytest.mark.performance
def test_precheck_performance():
    """Precheck should be fast (non-blocking)."""
    import time
    start = time.perf_counter()
    quick_precheck(5000, 300, 450, 500, 120, 25)
    elapsed_ms = (time.perf_counter() - start) * 1000
    assert elapsed_ms < 10  # Target: < 10ms (soft)

def test_precheck_is_advisory_only():
    """Precheck should never return ERROR-level severity."""
    result = quick_precheck(5000, 300, 450, 500, 120, 25)
    assert all(w.severity != Severity.ERROR for w in result.warnings)
```

---

### Feature 2: Sensitivity Analysis

**Module:** `structural_lib/insights/sensitivity.py`

**API:**
```python
@dataclass(frozen=True)
class SensitivityResult:
    """Sensitivity of one parameter."""
    parameter: str
    base_value: float
    perturbed_value: float
    base_utilization: float
    perturbed_utilization: float
    delta_utilization: float
    sensitivity: float  # delta_util / perturbation
    impact: str  # "low", "medium", "high"

@dataclass(frozen=True)
class RobustnessScore:
    """Overall design robustness assessment."""
    score: float  # 0-1 (higher = more robust)
    rating: str  # "excellent", "good", "acceptable", "poor"
    vulnerable_parameters: list[str]
    base_utilization: float
    sensitivity_count: int

def sensitivity_analysis(
    design_function: Callable,
    base_params: dict,
    parameters_to_vary: list[str],
    perturbation: float = 0.10
) -> tuple[list[SensitivityResult], RobustnessScore]:
    """
    Analyze parameter sensitivity via perturbation method.

    Args:
        design_function: Function that returns ComplianceCaseResult with governing_utilization
        base_params: Baseline design parameters
        parameters_to_vary: Which params to perturb (e.g., ['d_mm', 'b_mm', 'mu_knm'])
        perturbation: Perturbation magnitude (default 10%)

    Returns:
        (sensitivities, robustness_score)

    Method:
        One-at-a-time (OAT) perturbation:
        1. Perturb each parameter +10%
        2. Calculate change in governing_utilization
        3. Sensitivity = delta_utilization / perturbation
        4. Rank by |sensitivity|

    Limitations:
        - Assumes local linearity (valid for ±10%)
        - Doesn't capture parameter interactions
        - governing_utilization is proxy for safety margin

    Determinism:
        Pure function - same inputs always give same outputs
    """
```

**Implementation Safety:**

1. **Bounded perturbations** - Max ±10% to stay in linear region
2. **Validation** - Check perturbed designs are still valid
3. **Error handling** - If perturbed design fails, mark as "critical boundary"
4. **Clear interpretation** - Document what sensitivity means

**Robustness Calculation:**
```python
def calculate_robustness(
    sensitivities: list[SensitivityResult],
    base_utilization: float
) -> RobustnessScore:
    """
    Calculate design robustness score.

    Scoring logic:
        Base score = 1.0
        - Penalty for high-impact parameters (|sens| > 0.5): -0.15 each
        - Penalty for medium-impact (0.2 < |sens| ≤ 0.5): -0.05 each
        - Penalty for high base utilization (>50%): -0.2 * (util - 0.5)

    Thresholds:
        ≥ 0.80: Excellent (small parameter changes → minimal impact)
        ≥ 0.65: Good
        ≥ 0.50: Acceptable
        < 0.50: Poor (sensitive, consider increasing margins)

    Conservative bias:
        Err on side of lower scores (flag potential issues)
    """
    # Implementation from prototype
    ...
```

**Testing Strategy:**

```python
def test_sensitivity_depth_most_critical():
    """Depth should be more sensitive than width."""
    result = design_beam_is456(units="IS456", b_mm=300, d_mm=450, mu_knm=120, ...)

    sensitivities, _ = sensitivity_analysis(
        design_beam_is456,
        {'units': 'IS456', 'b_mm': 300, 'd_mm': 450, 'mu_knm': 120, ...},
        ['d_mm', 'b_mm']
    )

    sens_d = [s for s in sensitivities if s.parameter == 'd'][0]
    sens_b = [s for s in sensitivities if s.parameter == 'b'][0]

    # Depth should be more critical (lever arm effect)
    assert abs(sens_d.sensitivity) > abs(sens_b.sensitivity)

def test_sensitivity_deterministic():
    """Same inputs should give same sensitivity."""
    params = {'units': 'IS456', 'b_mm': 300, 'd_mm': 450, 'mu_knm': 120, ...}

    s1, r1 = sensitivity_analysis(design_beam_is456, params, ['d_mm', 'b_mm'])
    s2, r2 = sensitivity_analysis(design_beam_is456, params, ['d_mm', 'b_mm'])

    assert s1 == s2
    assert r1 == r2

def test_robustness_increases_with_margin():
    """Lower utilization → higher robustness."""
    # Design A: 80% utilization
    r_high = calculate_robustness(sensitivities_a, base_util=0.80)

    # Design B: 40% utilization (same sensitivities)
    r_low = calculate_robustness(sensitivities_a, base_util=0.40)

    assert r_low.score > r_high.score
```

---

### Feature 3: Constructability Scoring

**Module:** `structural_lib/insights/constructability.py`

**API:**
```python
@dataclass(frozen=True)
class ConstructabilityFactor:
    """One factor in constructability assessment."""
    factor: str  # "bar_spacing", "stirrup_spacing", "bar_variety"
    score: float  # Contribution to overall score
    penalty: float  # Negative if issue
    message: str  # Explanation
    recommendation: str  # How to improve

@dataclass(frozen=True)
class ConstructabilityScore:
    """Overall constructability assessment."""
    score: float  # 0-10 scale
    rating: str  # "excellent", "good", "acceptable", "poor"
    factors: list[ConstructabilityFactor]
    overall_message: str
    version: str  # Scoring algorithm version

def calculate_constructability_score(
    design_result: "ComplianceCaseResult",
    detailing: "BeamDetailingResult"
) -> ConstructabilityScore:
    """
    Assess construction ease on 0-10 scale.

    Based on IS 456 spacing limits + conservative heuristics.
    (External frameworks can be referenced, but rules stay deterministic.)

    Factors:
    1. Bar spacing (wider = better for concrete placement)
    2. Stirrup spacing (>125mm for vibrator access)
    3. Bar variety (fewer types = simpler procurement)
    4. Layer count (≤2 layers preferred)
    5. Standard sizes (12, 16, 20, 25mm bonus)
    6. Congestion ratio (total steel / available space) [optional stretch]

    Thresholds:
        ≥ 8.0: Excellent (typical site can easily execute)
        ≥ 6.0: Good (experienced crew recommended)
        ≥ 4.0: Acceptable (requires careful supervision)
        < 4.0: Poor (consider design changes)

    Regional Note:
        Thresholds based on Indian construction practice.
        Adjust for local conditions if needed.
    """
```

**Scoring Algorithm:**

```python
def calculate_constructability_score(design_result, detailing):
    score = 10.0  # Start perfect
    factors = []

    bars = detailing.top_bars + detailing.bottom_bars
    bar_sizes = [bar.diameter for bar in bars if bar.count > 0]
    min_clear_spacing = None
    max_layers = 1
    if bars:
        clear_spacings = [bar.spacing - bar.diameter for bar in bars if bar.spacing > 0]
        if clear_spacings:
            min_clear_spacing = min(clear_spacings)
        max_layers = max(bar.layers for bar in bars)

    min_stirrup_spacing = None
    if detailing.stirrups:
        min_stirrup_spacing = min(s.spacing for s in detailing.stirrups)

    # Factor 1: Bar spacing
    if min_clear_spacing is not None and min_clear_spacing < 40:
        penalty = 2.0
        factors.append(ConstructabilityFactor(
            factor="bar_spacing",
            score=0,
            penalty=-2.0,
            message=f"Clear spacing {min_clear_spacing:.0f}mm < 40mm (congested)",
            recommendation="Increase width or reduce bar diameter"
        ))
        score -= penalty
    elif min_clear_spacing is not None and min_clear_spacing < 60:
        penalty = 1.0
        factors.append(ConstructabilityFactor(
            factor="bar_spacing",
            score=0,
            penalty=-1.0,
            message=f"Clear spacing {min_clear_spacing:.0f}mm tight",
            recommendation="Consider spacing ≥60mm for easier placement"
        ))
        score -= penalty

    # Factor 2: Stirrup spacing
    if min_stirrup_spacing is not None and min_stirrup_spacing < 100:
        penalty = 2.0
        factors.append(ConstructabilityFactor(
            factor="stirrup_spacing",
            score=0,
            penalty=-2.0,
            message=f"Stirrup spacing {min_stirrup_spacing:.0f}mm < 100mm (very tight)",
            recommendation="Increase shear stirrup diameter or use higher fck"
        ))
        score -= penalty
    elif min_stirrup_spacing is not None and min_stirrup_spacing < 125:
        penalty = 1.5
        factors.append(ConstructabilityFactor(
            factor="stirrup_spacing",
            score=0,
            penalty=-1.5,
            message=f"Stirrup spacing {min_stirrup_spacing:.0f}mm < 125mm (tight for vibrator)",
            recommendation="Spacing ≥125mm allows better concrete vibration"
        ))
        score -= penalty

    # Factor 3: Bar variety
    unique_bars = len(set(bar_sizes + [s.diameter for s in detailing.stirrups]))
    if unique_bars > 2:
        penalty = 1.0
        factors.append(ConstructabilityFactor(
            factor="bar_variety",
            score=0,
            penalty=-1.0,
            message=f"{unique_bars} different bar sizes (procurement complexity)",
            recommendation="Limit to 2 bar sizes for simpler procurement"
        ))
        score -= penalty

    # Factor 4: Standard sizes bonus
    standard_sizes = {12, 16, 20, 25}
    if bar_sizes and all(size in standard_sizes for size in bar_sizes):
        bonus = 1.0
        factors.append(ConstructabilityFactor(
            factor="standard_sizes",
            score=1.0,
            penalty=0,
            message="All standard bar sizes (readily available)",
            recommendation=""
        ))
        score += bonus

    # Factor 5: Layer count
    if max_layers > 2:
        penalty = 1.0
        factors.append(ConstructabilityFactor(
            factor="layers",
            score=0,
            penalty=-1.0,
            message=f"{max_layers} layers used (congestion risk)",
            recommendation="Reduce bar count or increase width to keep ≤2 layers"
        ))
        score -= penalty

    # Clamp to 0-10
    score = max(0, min(10, score))

    # Rating
    if score >= 8.0:
        rating = "excellent"
    elif score >= 6.0:
        rating = "good"
    elif score >= 4.0:
        rating = "acceptable"
    else:
        rating = "poor"

    return ConstructabilityScore(
        score=score,
        rating=rating,
        factors=factors,
        overall_message=f"Constructability: {score:.1f}/10 ({rating})",
        version="1.0"
    )
```

**Implementation Safety:**

1. **Heuristic + cited** - Reference IS 456/SP 34; cite any external framework if used
2. **Conservative** - Penalize known construction issues
3. **Regional** - Document assumptions (Indian practice)
4. **Versioned** - Track algorithm version for reproducibility
5. **Advisory** - Never used as a pass/fail gate

**Testing Strategy:**

```python
def test_constructability_tight_spacing_penalty():
    """Tight spacing should reduce score."""
    # Design with 35mm bar spacing
    score_tight = calculate_constructability_score(design_tight, detailing_tight)

    # Design with 70mm bar spacing
    score_good = calculate_constructability_score(design_good, detailing_good)

    assert score_good.score > score_tight.score
    assert any("congested" in f.message for f in score_tight.factors)

def test_constructability_standard_sizes_bonus():
    """Standard sizes should get bonus."""
    # Design with 12, 16mm bars (standard)
    score_std = calculate_constructability_score(design_std, detailing_std)

    # Design with 14, 18mm bars (non-standard)
    score_nonStd = calculate_constructability_score(design_nonstd, detailing_nonstd)

    assert score_std.score > score_nonstd.score

def test_constructability_score_range():
    """Score should always be 0-10."""
    # Test 100 random valid designs
    for _ in range(100):
        design = generate_random_valid_design()
        score = calculate_constructability_score(design.result, design.detailing)
        assert 0 <= score.score <= 10
```

---

### Feature 4: Enhanced Verification (5-10 Benchmark Cases)

**Module:** `structural_lib/verification/` (data + runner, no network)

**Structure:**
```python
# verification/benchmark_cases.py

@dataclass(frozen=True)
class BenchmarkCase:
    """One verified benchmark problem."""
    case_id: str
    source: str  # "SP:16 Ex 3.1", "Textbook pg 245", "Hand calc"
    source_url: str  # Link if available (reference only; not fetched in tests)
    description: str
    inputs: dict
    expected_outputs: dict
    tolerances: dict
    clause_references: list[str]
    notes: str

# Example benchmark
BENCHMARK_CASES = [
    BenchmarkCase(
        case_id="SP16-Ex3.1",
        source="SP:16-1980 Example 3.1",
        source_url="https://archive.org/details/gov.in.is.sp.16.1980",
        description="Simply supported beam, rectangular, singly reinforced",
        inputs={
            'units': 'IS456',
            'b_mm': 230,
            'D_mm': 450,
            'd_mm': 400,
            'mu_knm': 95,
            'vu_kn': 65,
            'fck_nmm2': 20,
            'fy_nmm2': 415
        },
        expected_outputs={
            'flexure.ast_required': 804.5,  # mm²
            'flexure.is_safe': True,
            'shear.tv': 0.627,  # N/mm²
            'shear.spacing': 250  # mm
        },
        tolerances={
            'flexure.ast_required': 5.0,  # ±5 mm²
            'shear.tv': 0.01,  # ±0.01 N/mm²
            'shear.spacing': 10  # ±10 mm
        },
        clause_references=[
            "IS 456 Cl. 38.1 (limiting moment)",
            "IS 456 Cl. 40.1 (shear stress)",
            "IS 456 Table 19 (τc values)"
        ],
        notes="Classic SP:16 example, widely referenced"
    ),
    # ... 4-9 more cases
]
```

**Validation Runner:**
```python
# verification/validate.py

def validate_benchmark_case(case: BenchmarkCase) -> BenchmarkResult:
    """Run one benchmark case and compare to expected."""
    result = design_beam_is456(**case.inputs)

    mismatches = []
    for key, expected in case.expected_outputs.items():
        actual = get_nested_value(result, key)
        tolerance = case.tolerances.get(key, 0)

        if not approx_equal(actual, expected, tolerance):
            mismatches.append({
                'field': key,
                'expected': expected,
                'actual': actual,
                'tolerance': tolerance,
                'diff': abs(actual - expected)
            })

    return BenchmarkResult(
        case_id=case.case_id,
        passed=len(mismatches) == 0,
        mismatches=mismatches
    )

def validate_all_benchmarks() -> BenchmarkReport:
    """Validate entire benchmark corpus."""
    results = [validate_benchmark_case(c) for c in BENCHMARK_CASES]

    return BenchmarkReport(
        total=len(results),
        passed=sum(r.passed for r in results),
        failed=sum(not r.passed for r in results),
        results=results
    )
```

**Testing Strategy:**

```python
# tests/test_verification_pack.py

def test_all_benchmarks_pass():
    """All benchmark cases should pass."""
    report = validate_all_benchmarks()
    assert report.failed == 0, f"{report.failed} benchmark(s) failed"

def test_benchmark_determinism():
    """Repeated runs should give same results."""
    r1 = validate_benchmark_case(BENCHMARK_CASES[0])
    r2 = validate_benchmark_case(BENCHMARK_CASES[0])
    assert r1 == r2

def test_benchmark_coverage():
    """Benchmarks should cover key scenarios."""
    scenarios = set(c.description for c in BENCHMARK_CASES)

    # Must cover these scenarios
    required = {
        "singly reinforced",
        "doubly reinforced",
        "high shear",
        "flanged beam"
    }

    for req in required:
        assert any(req in s.lower() for s in scenarios), \
            f"Missing benchmark for: {req}"
```

**Benchmark Selection Criteria:**

1. **Source credibility** - SP:16, textbooks, verified hand calcs
2. **Scenario coverage** - Singly/doubly, high/low moment, shear-critical
3. **Clause coverage** - Exercise all major IS 456 clauses
4. **Practical relevance** - Common real-world cases (5m beam, M25, Fe500)
5. **Edge cases** - Near boundaries (Mu ≈ Mu,lim, τv ≈ τc,max)

**Target: 5-10 cases for v0.13:**
1. SP:16 Ex 3.1 - Singly reinforced, rectangular
2. SP:16 Ex 3.2 - Doubly reinforced
3. Textbook case - T-beam
4. Hand calc - Shear-critical beam
5. Edge case - Mu ≈ Mu,lim (transition point)
6-10. Additional scenarios based on common practice

---

## v0.13 Integration & API Changes

### API Integration (No Stable API Changes)

```python
# User explicitly calls insight features (opt-in)
from structural_lib.insights import precheck, sensitivity, constructability

pre = precheck.quick_precheck(span_mm=5000, b_mm=300, ...)

# Main design (stable API unchanged)
result = design_beam_is456(units="IS456", ...)

# Optional sensitivity/constructability
if result.is_ok:
    sens, robust = sensitivity.sensitivity_analysis(...)
    construct = constructability.calculate_constructability_score(result, detailing)
```

**v0.13 policy:** No new parameters on `design_beam_is456` and no new fields
on `ComplianceCaseResult`. Insights stay in their own module and outputs.

### CLI Integration

```bash
# Optional: separate insights command (non-blocking)
python -m structural_lib insights design_results.json -o insights.json
```

### JSON Output (Separate Insights Report)

```json
{
  "schema": "insights-v1",
  "version": "0.13.0",
  "case_id": "CASE-1",
  "precheck": { ... },
  "sensitivity": { ... },
  "constructability": { ... }
}
```

If we decide to embed insights into existing results JSON, we must bump the
results schema version and update `docs/reference/api.md`.

---

## v0.13 Testing Strategy

### Test Coverage Targets

| Module | Branch Coverage | Unit Tests | Integration Tests |
|--------|----------------|------------|-------------------|
| insights/ | ≥90% | 30+ | 10+ |
| verification/ | ≥95% | 15+ | 5+ |
| API integration | ≥85% | 10+ | 5+ |

### Test Categories

**1. Unit Tests** (fast, isolated)
```python
# Test individual functions
test_quick_precheck_span_depth_ratio()
test_sensitivity_calculation()
test_constructability_scoring()
test_robustness_calculation()
```

**2. Integration Tests** (full workflow)
```python
# Test with real design functions
test_precheck_before_design_workflow()
test_sensitivity_with_actual_beam()
test_constructability_with_detailing()
```

**3. Validation Tests** (benchmarks)
```python
# Test against known cases
test_all_benchmarks_pass()
test_benchmark_reproducibility()
```

**4. Property Tests** (invariants)
```python
# Test mathematical properties
test_sensitivity_sign_makes_sense()  # Increasing d_mm should decrease utilization
test_robustness_score_range()  # Always 0-1
test_constructability_score_range()  # Always 0-10
```

**5. Performance Tests**
```python
test_precheck_under_10ms()  # @pytest.mark.performance
test_sensitivity_scales_linearly()
```

### Regression Prevention

```python
# tests/test_v0_13_regression.py

def test_v0_12_designs_still_work():
    """Old code should produce same results."""
    # Load v0.12 test vectors
    vectors = load_v0_12_test_vectors()

    for vec in vectors:
        result_v0_13 = design_beam_is456(**vec.inputs)

        # Core outputs unchanged
        assert result_v0_13.flexure.ast_required == pytest.approx(vec.expected.ast_required)
        assert result_v0_13.shear.spacing == pytest.approx(vec.expected.spacing)

def test_insights_features_optional():
    """Insights features don't break old workflows."""
    # Old API call (no insights flags)
    result = design_beam_is456(units="IS456", mu_knm=120, vu_kn=100, ...)

    # Should work without insights fields
    assert result.flexure.is_safe  # Core functionality

    # No new fields added to core result type
    assert not hasattr(result, 'precheck')
```

---

## v0.13 Risk Mitigation

### Risk Matrix

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| Heuristics too conservative | Medium | Low | Test against 20+ cases, adjust thresholds |
| Sensitivity analysis slow | Low | Medium | Performance tests, optimize if needed |
| Breaking API changes | Low | High | Comprehensive regression tests |
| Insights mistaken for code checks | Medium | Medium | Disclaimers + separate outputs |
| Heuristic constructability scoring | Low | Medium | Cite assumptions, calibrate with examples |
| Benchmark mismatches | Medium | High | Start with well-known SP:16 cases |

### Rollback Plan

If v0.13.0 has critical issues:
1. Tag v0.12.1 as stable fallback
2. Deprecate v0.13.0 on PyPI
3. Release v0.13.1 with fixes
4. Document migration path

### Gradual Rollout

```
Week 1: Internal testing with sample cases
Week 2: Alpha release (internal stakeholders)
Week 3: Beta release (limited external users)
Week 4: Production release v0.13.0
```

---

## v0.13 Documentation Requirements

### User Documentation

1. **Getting Started Guide**
   - "Using Insights Features"
   - Quick examples for each feature
   - When to use what

2. **API Reference**
   - Full function signatures
   - Parameter descriptions
   - Return types with examples
   - Limitations and caveats

3. **Interpretation Guide**
   - "Understanding Sensitivity Results"
   - "What Does Constructability Score Mean?"
   - "Acting on Precheck Warnings"

### Developer Documentation

1. **Architecture Decisions**
   - Why separate modules?
   - Why heuristic approach?
   - Design trade-offs

2. **Testing Guide**
   - How to add benchmarks
   - How to validate heuristics
   - Regression test protocol

3. **Maintenance Guide**
   - Updating heuristic thresholds
   - Adding new constructability factors
   - Versioning insights algorithms

---

## v0.13 Timeline & Milestones

### Week 1: Core Implementation
- [ ] Day 1-2: Implement `quick_precheck()` + tests
- [ ] Day 3-4: Implement `sensitivity_analysis()` + tests
- [ ] Day 5: Implement `calculate_constructability_score()` + tests

### Week 2: Verification & Integration
- [ ] Day 1-3: Create 5-10 benchmark cases
- [ ] Day 4: Integrate into main API (optional flags)
- [ ] Day 5: CLI integration

### Week 3: Testing & Refinement
- [ ] Day 1-2: Comprehensive testing (unit + integration)
- [ ] Day 3: Performance optimization if needed
- [ ] Day 4-5: Calibrate heuristic thresholds against benchmarks

### Week 4: Documentation & Release
- [ ] Day 1-2: User documentation
- [ ] Day 3: Developer documentation
- [ ] Day 4: Release preparation (CHANGELOG, version bump)
- [ ] Day 5: v0.13.0 release + monitoring

**Slack:** +1 week for unexpected issues

---

# Part 2: v0.14 — Deterministic Alternatives

**Timeline:** 3-4 weeks
**Scope:** Deterministic alternatives search + detailing enhancements
**Risk Level:** Low-Medium (bounded search, no new deps)

---

## v0.14 Architecture

### Module Structure

```
structural_lib/
├── alternatives.py          # NEW: Deterministic alternatives search
│   ├── generate_alternatives()
│   ├── pareto_front()
│   ├── select_knee_point()
│   └── Supporting dataclasses
│
├── objectives.py            # NEW: Objective functions
│   ├── calculate_cost()
│   ├── calculate_carbon_footprint()
│   ├── calculate_weight()
│   └── Helpers
│
├── detailing.py            # ENHANCED: Anchorage checks
│   ├── check_anchorage_space()  # NEW
│   └── Existing functions
│
├── insights/               # STABLE (from v0.13)
└── types.py                # UNCHANGED (no new core fields)
```

### Design Principles

**1. Deterministic Enumeration**
```python
# Enumerate a discrete design space with stable ordering
result = generate_alternatives(..., max_evals=500)
pareto = result.pareto_front
```

**2. Objective Function Modularity**
```python
# Each objective is independent and testable
objectives = {
    'cost': lambda r, ctx: calculate_cost(r, **ctx),
    'weight': lambda r, ctx: calculate_weight(r, **ctx),
    'carbon': lambda r, ctx: calculate_carbon_footprint(r, **ctx),
    'safety': lambda r, _ctx: 1.0 - r.governing_utilization,
    'constructability': lambda r, ctx: calculate_constructability_score(r, ctx["detailing"]).score,
}

# User can select subset
generate_alternatives(..., objectives=['cost', 'safety'])
```
Note: `constructability` requires a `BeamDetailingResult`; omit it if detailing
is not available in the alternatives search.
`ctx` should include `b_mm`, `d_mm`, `D_mm`, `fck_nmm2`, `asv_mm2`, and
`detailing` when applicable.

**3. Bounded Search**
```python
# Enforce a hard cap on evaluations for predictable runtimes
alternatives = generate_alternatives(..., max_evals=500)
```

---

## v0.14 Implementation Details

### Feature 1: Deterministic Alternatives Search

**Dependencies:**
```
numpy>=1.21.0  # Already required
```

**API:**
```python
@dataclass(frozen=True)
class AlternativeDesign:
    """One feasible design candidate."""
    b_mm: float
    d_mm: float
    D_mm: float
    fck_nmm2: float
    fy_nmm2: float
    objectives: dict[str, float]
    design_result: "ComplianceCaseResult"
    rank: int  # Pareto rank (1 = non-dominated)

@dataclass(frozen=True)
class AlternativesResult:
    """Results from deterministic alternatives search."""
    pareto_front: list[AlternativeDesign]
    recommended_design: AlternativeDesign
    all_feasible: list[AlternativeDesign]
    objectives_used: list[str]
    evaluations: int
    search_space: dict

def generate_alternatives(
    *,
    units: str,
    mu_knm: float,
    vu_kn: float,
    span_mm: float,
    requirements: dict,
    objectives: list[str] | None = None,
    design_space: dict | None = None,
    max_evals: int = 500,
) -> AlternativesResult:
    """
    Enumerate a discrete design space deterministically.

    Args:
        units: "IS456" (explicit units)
        mu_knm: Factored moment
        vu_kn: Factored shear
        span_mm: Span length
        requirements: Constraints (max_depth, min_fck, max_cost, etc.)
        objectives: Which to score (default: ['cost', 'weight', 'safety'])
                    Use 'constructability' only if detailing results are available.
        design_space: Discrete lists for b/d/fck/fy (defaults to standard ranges)
        max_evals: Hard cap on design evaluations

    Returns:
        Pareto front + recommended design (knee point) from feasible candidates.

    Determinism:
        - No randomness, stable ordering.
        - Same inputs → same outputs.
    """
```

**Search Strategy:**

```python
def generate_alternatives(...):
    # Default discrete options (can be overridden)
    design_space = {
        "b_mm": [230, 250, 300, 350, 400],
        "d_mm": [400, 450, 500, 550, 600, 650, 700],
        "fck_nmm2": [20, 25, 30, 35, 40],
        "fy_nmm2": [415, 500, 550],
    }

    feasible = []
    for b in design_space["b_mm"]:
        for d in design_space["d_mm"]:
            D = d + requirements.get("cover_mm", 50)
            if D > requirements.get("max_depth", 1e9):
                continue

            for fck in design_space["fck_nmm2"]:
                if fck < requirements.get("min_fck", 15):
                    continue
                for fy in design_space["fy_nmm2"]:
                    if len(feasible) >= max_evals:
                        break

                    result = design_beam_is456(
                        units=units,
                        mu_knm=mu_knm,
                        vu_kn=vu_kn,
                        b_mm=b,
                        d_mm=d,
                        D_mm=D,
                        fck_nmm2=fck,
                        fy_nmm2=fy,
                        # ... other params
                    )
                    if not result.is_ok:
                        continue

                    objectives_map = score_objectives(
                        result,
                        b_mm=b,
                        d_mm=d,
                        D_mm=D,
                        fck_nmm2=fck,
                        fy_nmm2=fy,
                        asv_mm2=requirements.get("asv_mm2", 100),
                    )
                    feasible.append(AlternativeDesign(
                        b_mm=b,
                        d_mm=d,
                        D_mm=D,
                        fck_nmm2=fck,
                        fy_nmm2=fy,
                        objectives=objectives_map,
                        design_result=result,
                        rank=0,
                    ))

    pareto = pareto_front(feasible, objectives)
    recommended = select_knee_point(pareto, objectives)
    return AlternativesResult(
        pareto_front=pareto,
        recommended_design=recommended,
        all_feasible=feasible,
        objectives_used=objectives,
        evaluations=len(feasible),
        search_space=design_space,
    )
```

**Knee Point Selection:**

```python
def select_knee_point(pareto_designs, objectives):
    """
    Find the 'knee' point on Pareto front (best compromise).

    Method: Maximum distance from ideal/nadir line

    The knee point balances all objectives - not best in any single
    objective, but best overall trade-off.
    """
    # Normalize objectives to [0, 1]
    obj_values = {obj: [d.objectives[obj] for d in pareto_designs]
                  for obj in objectives}

    normalized = []
    for design in pareto_designs:
        norm_obj = {}
        for obj in objectives:
            min_val = min(obj_values[obj])
            max_val = max(obj_values[obj])
            norm_obj[obj] = (design.objectives[obj] - min_val) / (max_val - min_val)
        normalized.append(norm_obj)

    # Find design with max distance from diagonal
    distances = []
    for norm in normalized:
        # Distance from ideal point (all zeros in normalized space)
        dist = sum(v**2 for v in norm.values()) ** 0.5
        distances.append(dist)

    # Knee = minimum distance (closest to ideal)
    knee_idx = distances.index(min(distances))

    return pareto_designs[knee_idx]
```

---

### Feature 2: Objective Functions

**Module:** `objectives.py`

**Cost Calculation:**
```python
def calculate_cost(
    design_result: "ComplianceCaseResult",
    b_mm: float,
    d_mm: float,
    D_mm: float,
    fck_nmm2: float,
    asv_mm2: float,
    constructability: "ConstructabilityScore | None" = None,
    unit_rates: dict = None
) -> float:
    """
    Calculate beam cost per meter.

    Components:
    1. Concrete volume × rate
    2. Steel weight × rate
    3. Formwork area × rate
    4. Labor (function of complexity)

    Args:
        unit_rates: {'concrete': 6500, 'steel': 75000, 'formwork': 450, 'labor_base': 1200}
                   Default rates in ₹ (2024 Indian market rates)
        asv_mm2: Shear reinforcement area used for weight estimate (mm²)
        constructability: Optional precomputed score (if available)

    Returns:
        Cost in ₹/m

    Regional Note:
        Default rates for India. Override for other regions.
    """
    if unit_rates is None:
        unit_rates = {
            'concrete': 6500,  # ₹/m³ (M25 delivered + placement)
            'steel': 75000,    # ₹/tonne (Fe500 with fabrication)
            'formwork': 450,   # ₹/m² (rental + labor)
            'labor_base': 1200  # ₹/m (base tying labor)
        }

    # 1. Concrete cost
    volume_m3 = (b_mm * D_mm) / 1e6  # Per meter length
    concrete_cost = volume_m3 * unit_rates['concrete']

    # 2. Steel cost
    steel_weight_kg = calculate_weight(
        design_result,
        asv_mm2=asv_mm2,
        stirrup_spacing_mm=design_result.shear.spacing,
        b_mm=b_mm,
        d_mm=d_mm,
    )
    steel_cost = (steel_weight_kg / 1000) * unit_rates['steel']

    # 3. Formwork cost
    formwork_area_m2 = 2 * ((b_mm + D_mm) / 1000)  # Perimeter
    formwork_cost = formwork_area_m2 * unit_rates['formwork']

    # 4. Labor cost (increases with complexity)
    if constructability is None:
        labor_multiplier = 1.0  # Default if no constructability score
    else:
        labor_multiplier = 2.0 - (constructability.score / 10)  # 1.0x to 2.0x
    labor_cost = unit_rates['labor_base'] * labor_multiplier

    total = concrete_cost + steel_cost + formwork_cost + labor_cost

    return total

def calculate_carbon_footprint(
    design_result: "ComplianceCaseResult",
    b_mm: float,
    d_mm: float,
    D_mm: float,
    fck_nmm2: float,
    asv_mm2: float,
    emission_factors: dict = None
) -> float:
    """
    Calculate carbon footprint per meter.

    Emission factors (kg CO2e per unit):
    - Concrete: 250 kg/m³ (typical for M25)
    - Steel: 2.5 kg/kg (embodied carbon)

    Args:
        emission_factors: Override default values
        asv_mm2: Shear reinforcement area used for weight estimate (mm²)

    Returns:
        Carbon footprint in kg CO2e per meter

    Source:
        Emission factors from ICE database (UK), conservative estimates
    """
    if emission_factors is None:
        emission_factors = {
            'concrete_m25': 250,  # kg CO2e / m³
            'concrete_m30': 275,  # Higher grade = more cement
            'concrete_m35': 300,
            'steel': 2.5  # kg CO2e / kg steel
        }

    # Concrete emissions
    volume_m3 = (b_mm * D_mm) / 1e6
    concrete_factor = emission_factors.get(f'concrete_m{int(fck_nmm2)}', 250)
    concrete_emissions = volume_m3 * concrete_factor

    # Steel emissions
    steel_weight_kg = calculate_weight(
        design_result,
        asv_mm2=asv_mm2,
        stirrup_spacing_mm=design_result.shear.spacing,
        b_mm=b_mm,
        d_mm=d_mm,
    )
    steel_emissions = steel_weight_kg * emission_factors['steel']

    total = concrete_emissions + steel_emissions

    return total

def calculate_weight(
    design_result: "ComplianceCaseResult",
    *,
    asv_mm2: float,
    stirrup_spacing_mm: float,
    b_mm: float,
    d_mm: float,
) -> float:
    """
    Calculate total steel weight per meter.

    Includes:
    - Main flexural reinforcement
    - Shear stirrups
    - Side-face bars (if any)

    Returns:
        Weight in kg/m
    """
    # Flexural steel
    ast_total = design_result.flexure.ast_required  # mm²
    if design_result.flexure.asc_required > 0:
        ast_total += design_result.flexure.asc_required

    # Convert area to weight (ρ_steel = 7850 kg/m³)
    main_weight = (ast_total / 1e6) * 1.0 * 7850  # kg/m

    # Stirrups weight
    # Asv per stirrup × (1000mm / spacing)
    stirrups_per_m = 1000 / stirrup_spacing_mm

    # Stirrup length ≈ perimeter
    stirrup_length_m = 2 * (b_mm + d_mm) / 1000

    stirrup_weight = (asv_mm2 / 1e6) * stirrup_length_m * stirrups_per_m * 7850

    total_weight = main_weight + stirrup_weight

    return total_weight

def get_safety_margin(design_result: "ComplianceCaseResult") -> float:
    """
    Calculate overall safety margin.

    Safety margin = 1 - governing_utilization

    Returns:
        0 = at limit, 1 = infinitely safe
    """
    return 1.0 - design_result.governing_utilization
```

**Testing Strategy:**

```python
def test_cost_increases_with_size():
    """Larger beam should cost more."""
    small = design_beam_is456(units="IS456", b_mm=230, d_mm=400, ...)
    large = design_beam_is456(units="IS456", b_mm=300, d_mm=500, ...)

    cost_small = calculate_cost(small, 230, 400, 450, fck_nmm2=25, asv_mm2=100)
    cost_large = calculate_cost(large, 300, 500, 550, fck_nmm2=25, asv_mm2=100)

    assert cost_large > cost_small

def test_carbon_proportional_to_weight():
    """Carbon should increase with steel weight."""
    light = design_beam_is456(units="IS456", mu_knm=80, ...)  # Low steel
    heavy = design_beam_is456(units="IS456", mu_knm=160, ...)  # High steel

    carbon_light = calculate_carbon_footprint(light, ..., fck_nmm2=25, asv_mm2=100)
    carbon_heavy = calculate_carbon_footprint(heavy, ..., fck_nmm2=25, asv_mm2=100)

    assert carbon_heavy > carbon_light

def test_weight_calculation_accuracy():
    """Weight should match hand calculation."""
    result = design_beam_is456(units="IS456", ...)

    # Hand calc: Ast=800mm², Asv=100mm² @ 150mm, 1m length
    expected_weight_kg = (
        (800 / 1e6) * 1.0 * 7850 +  # Main steel
        (100 / 1e6) * 2 * (300 + 450) / 1000 * (1000 / 150) * 7850  # Stirrups
    )

    actual = calculate_weight(
        result,
        asv_mm2=100,
        stirrup_spacing_mm=result.shear.spacing,
        b_mm=300,
        d_mm=450,
    )
    assert actual == pytest.approx(expected_weight_kg, rel=0.05)

def test_alternatives_finds_pareto_front():
    """Alternatives search should return non-dominated solutions."""
    result = generate_alternatives(
        units="IS456",
        mu_knm=120,
        vu_kn=100,
        span_mm=5000,
        requirements={'max_depth': 600},
        objectives=['cost', 'weight'],
        max_evals=200,
    )

    # All solutions should be Pareto optimal (rank 1)
    assert all(d.rank == 1 for d in result.pareto_front)

    # No solution dominates another
    for d1 in result.pareto_front:
        for d2 in result.pareto_front:
            if d1 != d2:
                assert not dominates(d1, d2, ['cost', 'weight'])

def test_alternatives_deterministic():
    """Same inputs should give same Pareto front."""
    r1 = generate_alternatives(units="IS456", ..., max_evals=200)
    r2 = generate_alternatives(units="IS456", ..., max_evals=200)

    # Same designs (allow small numerical differences)
    assert len(r1.pareto_front) == len(r2.pareto_front)
    for d1, d2 in zip(r1.pareto_front, r2.pareto_front):
        assert d1.b_mm == pytest.approx(d2.b_mm, abs=1)
        assert d1.d_mm == pytest.approx(d2.d_mm, abs=1)

def test_knee_point_is_reasonable():
    """Knee point should be in middle of Pareto front."""
    result = generate_alternatives(units="IS456", ..., objectives=['cost', 'weight'])

    costs = [d.objectives['cost'] for d in result.pareto_front]
    weights = [d.objectives['weight'] for d in result.pareto_front]

    knee_cost = result.recommended_design.objectives['cost']
    knee_weight = result.recommended_design.objectives['weight']

    # Knee should not be at extremes
    assert knee_cost not in [min(costs), max(costs)]
    assert knee_weight not in [min(weights), max(weights)]
```

---

### Feature 3: Detailing Enhancements

**Anchorage Space Check (IS 456 Cl. 26.2):**

```python
# detailing.py

def check_anchorage_space(
    bar_dia_mm: float,
    n_bars: int,
    b_mm: float,
    cover_mm: float,
    stirrup_dia_mm: float = 8
) -> tuple[bool, str]:
    """
    Check if bars can be anchored with required spacing.

    IS 456 Cl. 26.3.3: Minimum spacing = max(bar_dia, aggregate_size + 5mm, 20mm)
    Typical aggregate size: 20mm

    Available width = b - 2×cover - 2×stirrup_dia
    Required width = n_bars × bar_dia + (n_bars - 1) × spacing

    Returns:
        (is_adequate, message)
    """
    # Minimum spacing per IS 456
    min_spacing = max(bar_dia_mm, 25, 20)  # 20mm aggregate + 5mm

    # Available width
    effective_width = b_mm - 2 * cover_mm - 2 * stirrup_dia_mm

    # Required width
    required_width = n_bars * bar_dia_mm + (n_bars - 1) * min_spacing

    is_adequate = effective_width >= required_width

    if is_adequate:
        actual_spacing = (effective_width - n_bars * bar_dia_mm) / (n_bars - 1)
        message = f"OK: {actual_spacing:.0f}mm spacing (≥{min_spacing:.0f}mm required)"
    else:
        shortage = required_width - effective_width
        message = f"FAIL: {shortage:.0f}mm short. Increase width or reduce bar count."

    return is_adequate, message
```

**Testing:**

```python
def test_anchorage_adequate_spacing():
    """Wide beam should have adequate spacing."""
    ok, msg = check_anchorage_space(
        bar_dia_mm=20, n_bars=3, b_mm=300,
        cover_mm=40, stirrup_dia_mm=8
    )
    assert ok
    assert "OK" in msg

def test_anchorage_inadequate_spacing():
    """Narrow beam should fail."""
    ok, msg = check_anchorage_space(
        bar_dia_mm=25, n_bars=4, b_mm=230,
        cover_mm=40, stirrup_dia_mm=8
    )
    assert not ok
    assert "FAIL" in msg
```

---

## v0.14 Timeline & Milestones

### Week 1: Alternatives Core
- [ ] Day 1: Define discrete design space + constraints
- [ ] Day 2-3: Implement `generate_alternatives()` + Pareto filter
- [ ] Day 4: Implement objective functions (cost, weight, carbon)
- [ ] Day 5: Test basic alternatives workflow

### Week 2: Integration & Polish
- [ ] Day 1-2: Integrate with v0.13 insights (optional)
- [ ] Day 3: Implement knee point selection
- [ ] Day 4: Trade-off analysis
- [ ] Day 5: CLI integration

### Week 3: Detailing & Testing
- [ ] Day 1: Anchorage space check
- [ ] Day 2: Enhanced error hints (using v0.13 precheck, no error-code changes)
- [ ] Day 3-4: Comprehensive testing
- [ ] Day 5: Performance optimization

### Week 4: Documentation & Release
- [ ] Day 1-2: User guide ("How to use alternatives")
- [ ] Day 3: API documentation
- [ ] Day 4: Release prep
- [ ] Day 5: v0.14.0 release

**Slack:** +1 week for unexpected issues

---

## v0.14 Risk Mitigation

### Risk Matrix

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| Search space too large | Medium | Medium | Cap `max_evals`, prune early |
| Alternatives too slow | Medium | Medium | Performance testing, parameter tuning |
| Pareto front too large | Low | Low | Limit to 20 designs, clustering |
| Cost/carbon estimates inaccurate | Medium | Low | Document assumptions, allow overrides |
| Constructability needs detailing | Medium | Low | Only include objective when detailing is available |
| Knee point selection suboptimal | Medium | Low | Multiple methods, user can override |

### Dependency Safety

```python
# No new dependencies in v0.14.
# If a stochastic solver is added later, it must be optional and gated behind extras.
```

**pyproject.toml:**
```toml
[project.optional-dependencies]
alternatives = []  # Placeholder if optional deps are introduced later
full = []  # All optional features
```

---

## Cross-Version Stability

### API Stability Contract

**Stable (won't change):**
- Core design functions (`design_singly_reinforced`, `design_beam_is456`)
- Result dataclass fields (existing fields)
- Error codes and severity levels

**Additive only:**
- New insights module (opt-in, separate outputs)
- New alternatives functions (opt-in)
- New CLI commands (no breaking changes to existing)

**Can change (documented):**
- Heuristic thresholds (with version tracking)
- Objective function coefficients (regional variations)
- Alternatives search parameters (design space, max_evals)

### Semantic Versioning

```
v0.13.0 - Minor (new features, backward compatible)
v0.13.1 - Patch (bug fixes, no API changes)
v0.14.0 - Minor (alternatives engine, backward compatible)
```

**Breaking changes reserved for v1.0+**

---

## Success Criteria

### v0.13 Success Metrics

- [ ] All 5-10 benchmark cases pass (100% accuracy)
- [ ] Precheck < 10ms (performance)
- [ ] Sensitivity analysis identifies depth as most critical (validation)
- [ ] Constructability scores correlate with expert judgment (user testing)
- [ ] Zero breaking changes to v0.12 API (regression tests pass)
- [ ] Documentation complete (user + developer guides)
- [ ] Test coverage ≥90% (branch coverage)

### v0.14 Success Metrics

- [ ] Alternatives search finds Pareto front in <30s (performance)
- [ ] Deterministic results (same inputs → same designs)
- [ ] Knee point design has balanced objectives (validation)
- [ ] Cost estimates within ±15% of market rates (calibration)
- [ ] All Pareto designs are IS 456 compliant (safety)
- [ ] No new mandatory dependencies (graceful for optional add-ons)
- [ ] Documentation complete with examples
- [ ] Test coverage ≥85%

---

## Rollout Strategy

### Alpha Testing (Week 3 of each release)
- Internal testing with 10-20 real cases
- Calibrate thresholds/coefficients
- Fix critical bugs

### Beta Testing (Week 4 of each release)
- External testers (2-3 engineers)
- Gather feedback on usability
- Validate against practice

### Production Release
- Tag release on GitHub
- Publish to PyPI
- Announce on mailing list/forum
- Monitor for issues (24-48 hour watch)

### Maintenance
- Bug fixes as v0.X.1, v0.X.2 patches
- Monthly check-in on issues
- Quarterly review of heuristics/coefficients

---

## Documentation Plan

### For v0.13

**User Docs:**
1. "Quick Start: Insights Features"
2. "Understanding Sensitivity Results"
3. "Acting on Precheck Warnings"
4. "Constructability Score Guide"
5. API Reference (full function signatures)

**Developer Docs:**
1. "Architecture: Insights Module"
2. "Adding Benchmark Cases"
3. "Calibrating Heuristics"
4. "Testing Guide"

### For v0.14

**User Docs:**
1. "Alternatives Guide: Finding Feasible Designs"
2. "Interpreting Pareto Fronts"
3. "Cost and Carbon Estimation"
4. "Trade-off Analysis"
5. API Reference (alternatives module)

**Developer Docs:**
1. "Architecture: Alternatives Engine"
2. "Adding Custom Objectives"
3. "Tuning Search Parameters (design space, max_evals)"
4. "Testing Alternatives Search"

---

## Summary

### v0.13 Deliverables (3-4 weeks)

**Code:**
- `insights/` - Predictive validation, sensitivity, constructability
- `verification/` - 5-10 benchmark cases
- Optional CLI command (`insights`)

**Tests:**
- 55+ unit tests
- 15+ integration tests
- 5-10 benchmark validation tests
- Regression tests

**Docs:**
- 5 user guides
- 4 developer guides
- Full API reference

### v0.14 Deliverables (3-4 weeks)

**Code:**
- `alternatives.py` - Deterministic alternatives search
- `objectives.py` - Cost, weight, carbon calculations
- Enhanced `detailing.py` - Anchorage checks
- API integration (opt-in utilities)

**Tests:**
- 40+ unit tests
- 10+ integration tests
- Performance benchmarks
- Determinism tests

**Docs:**
- 5 user guides
- 4 developer guides
- Full API reference
- Example notebooks

---

**Total Timeline:** 6-8 weeks for both releases
**Risk Level:** Low-Medium (mature algorithms, optional features)
**Impact:** High (game-changing capabilities)

---

*Document Version: 1.0*
*Created: 2025-12-30*
*Status: Ready for review and approval*
